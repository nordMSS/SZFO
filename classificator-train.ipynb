{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d0bd92-9a11-4725-969f-24124b3d1432",
   "metadata": {},
   "source": [
    "Импортируем все нужные нам библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5d2492-bbc7-43c7-8fb7-3531f787a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "SEED = 2005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77d4be8-c138-47c3-8f2a-1738ad91a42e",
   "metadata": {},
   "source": [
    "Инициализируем генератор случайных чисел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da542e76-2773-4f91-b9a0-247fec086130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79039bb5-6f00-4bd9-a156-1211ec9551b9",
   "metadata": {},
   "source": [
    "Создадим трансформации изображений для тренировочной и валидационной выборок.\n",
    "\n",
    "Изображения тренировочной выборки будем поворачивать, брать случайные фрагменты изображения, изменять яркость/контраст/насыщенность.\n",
    "\n",
    "Для создания датасета воспользуемся встроенной библиотекой из пакета torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af320fa-25f7-428d-a0f2-b863f7c5cdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset ImageFolder\n",
       "     Number of datapoints: 18834\n",
       "     Root location: train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                RandomRotation(degrees=[-8.0, 8.0], interpolation=nearest, expand=False, fill=0)\n",
       "                RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                RandomVerticalFlip(p=0.1)\n",
       "                ColorJitter(brightness=(0.9, 1.1), contrast=(0.9, 1.1), saturation=(0.9, 1.1), hue=(-0.03, 0.03))\n",
       "                PILToTensor()\n",
       "                ConvertImageDtype()\n",
       "                Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "                RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0, inplace=False)\n",
       "            ),\n",
       " Dataset ImageFolder\n",
       "     Number of datapoints: 18834\n",
       "     Root location: train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                PILToTensor()\n",
       "                ConvertImageDtype()\n",
       "                Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       "            ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomRotation(8),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.90, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(p=0.1),\n",
    "    transforms.ColorJitter(brightness=0.10, contrast=0.10, saturation=0.10, hue=0.03),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    transforms.RandomErasing(scale=(0.02, 0.10))]\n",
    ")\n",
    "\n",
    "transforms_valid = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
    ")\n",
    "\n",
    "dataset_train = datasets.ImageFolder('train', transform = transforms_train)\n",
    "dataset_valid = datasets.ImageFolder('train', transform = transforms_valid)\n",
    "\n",
    "dataset_train, dataset_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9afcfec-f0c6-4e21-abdd-6cec880cde6a",
   "metadata": {},
   "source": [
    "Посмотрим на наши классы изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348f44cc-5788-4749-950c-737fe53cc0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f3382-7011-439e-a768-ffb79422507b",
   "metadata": {},
   "source": [
    "Возьмем предобученную модель VisionTransformer, заменим в ней классификационный слой и инициализируем его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac1cd9a-6b26-4eea-9631-be653b751fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1)\n",
    "model.heads.head = torch.nn.Linear(768, len(dataset_train.classes))\n",
    "\n",
    "torch.nn.init.xavier_uniform_(model.heads.head.weight)\n",
    "torch.nn.init.constant_(model.heads.head.bias, 0.0)\n",
    "model.to('cuda')\n",
    "\n",
    "model.heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da3a3f-22e8-4996-ba56-bb1e95223135",
   "metadata": {},
   "source": [
    "Создадим даталоадеры, которые будут бить данные на батчи и отдавать модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658ed81b-8ec7-487a-824c-3251a35f26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=32,\n",
    "                                           num_workers=4, shuffle=True,\n",
    "                                           drop_last=True, pin_memory=True)\n",
    "loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=32,\n",
    "                                           num_workers=4, shuffle=False,\n",
    "                                           drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57cd77-b7b0-4acc-9c04-534a14449133",
   "metadata": {},
   "source": [
    "Так как классы не сбалансированы, то для функции потерь посчитаем корректировочные коэффициенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275a73f6-4089-4673-8cf1-b681e522684e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0189], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = len(dataset_train.targets) / (len(np.unique(dataset_train.targets)) * np.bincount(dataset_train.targets))\n",
    "weight = torch.FloatTensor(weight)\n",
    "weight = torch.nan_to_num(weight, posinf=1.0, neginf=1.0)\n",
    "weight = weight.to('cuda') / weight.min()\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54ba8b-b55c-4ddb-8eef-b6f9d1fc22fe",
   "metadata": {},
   "source": [
    "Замораживаем предобученные слои и одну эпоху учим только наш классификационный слой. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73a5e38a-8620-4bd1-96e8-f087360711af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 3 s, total: 1min 34s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# pretrain\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.heads.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight)\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "for imgs, label in loader_train:\n",
    "    pred = model(imgs.to('cuda'))\n",
    "    loss = criterion(pred, label.to('cuda'))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268a171-197b-47ad-92c1-a22cf9f89f07",
   "metadata": {},
   "source": [
    "Основной цикл вычислений. Размораживаем все слои и учим модель 15 эпох.\n",
    "В качестве шедулера возьмем OneCycleLR - он плавно повышает LR до целевого значения и потом сильно понижает его.\n",
    "Во время валидации проверяем качество метрики F1. Если метрика не повышается несколько эпох, то загружаем в модель предыдущие лучшие веса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded6cc6-477d-4aa1-b1e3-8ae4b2e017a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1 at 17:51:38, lr=0.00000050\n",
      "  train loss: 0.0336 valid f1: 0.9909\n",
      "Saved best model!\n",
      "Start epoch 2 at 17:56:45, lr=0.00000497\n",
      "  train loss: 0.0312 valid f1: 0.9998\n",
      "Saved best model!\n",
      "Start epoch 3 at 18:01:52, lr=0.00000407\n",
      "  train loss: 0.0066 valid f1: 1.0000\n",
      "Saved best model!\n",
      "Start epoch 4 at 18:06:58, lr=0.00000234\n",
      "  train loss: 0.0004 valid f1: 1.0000\n",
      "Start epoch 5 at 18:12:04, lr=0.00000071\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, epochs=epochs, max_lr=5e-6,\n",
    "                                                   div_factor=10.0, final_div_factor=10.0,\n",
    "                                                   steps_per_epoch=1)\n",
    "\n",
    "best_f1  = 0\n",
    "best_cnt = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Start epoch {epoch + 1} at {datetime.now().strftime('%H:%M:%S')}, lr={current_lr:0.8f}\")\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_losses = []\n",
    "    for imgs, label in loader_train:\n",
    "        pred = model(imgs.to('cuda'))\n",
    "        loss = criterion(pred, label.to('cuda'))\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    val_label = []\n",
    "    trg_label = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, label in loader_valid:\n",
    "            pred = model(imgs.to('cuda'))\n",
    "            trg_label.extend(label.numpy().tolist())\n",
    "            val_label.extend(pred.argmax(dim=1).cpu().numpy().flatten().tolist())\n",
    "    f1_label = f1_score(trg_label, val_label, zero_division=0, average='macro')\n",
    "    \n",
    "    print(f\"  train loss: {np.mean(train_losses):6.4f} valid f1: {f1_label:6.4f}\")\n",
    "\n",
    "    if f1_label > best_f1:\n",
    "        best_f1 = f1_label\n",
    "        best_cnt = 0\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "        print(\"Saved best model!\")\n",
    "    else:\n",
    "        best_cnt += 1\n",
    "\n",
    "    if best_cnt > 2:\n",
    "        print(\"Loading best model weights!\")\n",
    "        model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd0720-64dd-4f5b-bef1-d3266f17a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_LB_xx.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea0e59-f6e8-4187-aaa3-2329eccb2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_idx0 = {v:k for k, v in dataset_train.class_to_idx.items()}\n",
    "rev_idx0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854ef37-975d-41e4-aec4-3cc257ac8f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
